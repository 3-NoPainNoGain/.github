# 2차 보고서 
## Team-Info
| (1) 과제명 |  청각장애인 대상 수어/음성 텍스트 변환 대면+비대면 진료 서비스 |
|:---  |---  |
| (2) 팀 번호/이름 | 03-노페인노게인 |
| (3) 구성원 | 최수희(2271104): 리더, AI, Backend <br> 구혜준(2276019): 팀원, Backend, Frontend, AI <br> 이하은(2276253): 팀원, Frontend |
| (4) 지도교수 | 심재형 교수 |
| (5) 트랙  | 산학 |
| (6) 과제 키워드 | 수어 통역, STT(Speech-to-Text), 대면+비대면 진료, 배리어 프리 |
| (7) 과제 내용 요약 | handDoc은 수어와 음성을 텍스트로 변환하는 기술을 활용하여, 청각장애인의 대면 진료와 비대면 진료 모두에서 보다 편리하게 소통할 수 있도록 돕는 서비스입니다. </br> </br> 1. 수어를 사용하는 청각장애인의 수어를 인식하여 텍스트로 변환합니다. </br>2. 의사의 음성 발화를 텍스트로 변환합니다. <br> 3. 수어 외에도 음성 발화를 활용하는 청각장애인의 구음을 텍스트로 변환합니다. </br> 4. 진료 과정에서 오간 수어와 음성 대화를 텍스트로 정리하고 요약하여 제공합니다. <br> 5. 서비스를 사용하는 병원의 위치정보를 지도에 표시합니다. |

<br> 

## Project-Summary 
| 항목 | 내용 |
|:-----  |-----  |
| (1) 문제 정의 | **[배경]** <br> - 보건복지부에 따르면, 2024년 말 기준 우리나라 등록 장애인 약 263만명 중 16.3%가 청각장애인 <br> - 수어를 주된 의사소통 방법으로 사용하는 청각장애인은 30.1%에 해당 <br> - 국립국어원 2023년 한국수어 활용 조사에 따르면, 청각장애인이 수어 통역 서비스가 가장 필요하다고 생각하는 영역이 의료기관이 83.0%로 가장 높음 <br> - 국회복지부 자료 분석 보도에 따르면, 2021년 8워 기준 수어통역사가 상주하는 의료기관은 2개소에 불과함 <br> - 코로나19 시기 임시 허용을 기점으로 비대면 진료 이용이 급증 <br> - 2023년 시범사업 전환 이후에도 이용은 지속 증가했고, 누적 492만명 이상이 최소 1회 이용  <br> <br> **[문제 정의]** <br> - 현 비대면 진료는 음성 통화 중심의 흐름으로 구성되어 청각장애인의 원활한 의사소통을 보장하지 못함 <br> - 대면 진료 시에도 제 3자의 도움을 받거나 필담에 의존해야 하는 소통의 어려움이 존재하지만, 의료 기관 내 수어 통역사 부족으로 상황은 개선되지 못하고 있음 <br> - 진료 환경에서의 청각장애인 친화적인 도구가 부재하여, 결과적으로 의료 불평등이 심화됨 |
| (2) 기존 연구와의 비교 | **[타 비대면진료 서비스와의 비교]** <br><br> 1.  닥터나우 <br> - 진료 채널 : 화상/음성 통화 <br> - 수어/음성-텍스트 변환 기능 : 없음 <br> - 환자 선택권 : 모든 기능이 동일하게 적용되어 개인 상황 고려 부족 <br> - 진료 기록 : 진료 상세에서 기본 정보 확인 가능 (병원/의사, 일시, 방식) <br> - 장점 : 주치의 개념으로 지속적인 관리가 가능하고 개인화된 상담이 용이 <br> - 단점 : 청각장애인의 접근성 부족, 수어/자막 기능 부재 <br><br>  2. 나만의 닥터 <br> - 진료 채널 : 화상/음성 통화 <br> - 수어/음성-텍스트 변환 기능 : 없음 <br> - 환자 선택권 : 모든 기능 동일하게 적용, 접근성 제한 (즉시 연결보다는 특정 병원/의사에 묶여있기 때문에 가입이나 이용의 진입장벽이 비교적 높음)  <br> - 진료 기록 : 진료 상세에서 기본 정보 확인 가능 (병원/의사, 일시, 방식) <br> - 장점 : 신속한 연결, 약 배송 서비스에 강점 <br> - 단점 : 접근성이 낮고 청각장애인 지원 기능이 없음  <br><br> 기존의 비대면 진료 서비스는 원격 환경에서 환자가 편리하고 신속하게 진료를 받는 것에 초점이 맞추어져 있으나, 청각장애인의 소통권 보장 측면에서는 한계가 존재합니다. 이에 비해 본 서비스(handDoc)는 청각장애인의 의료 접근권 보장을 최우선 가치로 삼으며, 비대면 진료뿐만 아니라 대면 진료 환경에서도 활용 가능한 보조 도구로 기능합니다. 환자는 의사의 발화를 실시간 텍스트로 확인할 수 있고, 자신의 수어 표현을 텍스트 음성으로 변환하여 전달할 수 있어, 의료 환경 전반에서 동등한 소통권을 보장받게 된다. 따라서 handDoc은 청각장애인에게 원활한 의사소통 수단과 의료 접근권을 제공하고 보장한다는 점에서 기존 서비스와 근본적인 차별성을 갖습니다. |
| (3) 제안 내용 | 청각장애인의 의료 접근성 향상을 위해 **'청각장애인 대상 수어/음성-텍스트 변환 기반 대면 + 비대면 진료 서비스'** 를 제안합니다. <br><br> 1. 환자 수어 인식 및 통역 : Google Mediapipe Holistic을 활용하여 얼굴/손/신체의 주요 키포인트를 추출한 뒤, 이를 Bi-LSTM 기반 수어 인식 모델에 입력하여 학습된 결과를 텍스트로 변환합니다. 이를 통해 의사는 환자의 수어 표현을 실시간으로 확인할 수 있습니다. <br> 2. 의사의 음성 자막화 : 의사의 발화를 Naver Clova Speech-to-Text API를 활용하여 자막 형태로 제공합니다. <br> 3. 진료 기록 및 요약 : 진료 종료 후에는 OpenAI gpt-4o 모델을 활용해 대화 내용을 요약합니다. 요약본은 환자에게 제공되어 진료 내용을 다시 확인할 수 있도록 돕고, 환자의 치료 과정에 대한 이해도와 신뢰도를 높입니다. <br> 4. 수어가 아닌 구음을 사용하는 청각장애인의 음성 텍스트 변환 : 수어가 아닌 구음을 사용하는 청각장애인의 불명확한 음성을 파인튜닝한 Whisper 모델을 통해 텍스트화합니다. 이 텍스트를 OpenAI gpt-4o 모델에 전달하여 3가지 문장 선택지를 생성합니다. 환자는 3가지 선택지 중 자신의 의도와 가장 일치하는 문장을 선택하여 의사에게 전송합니다. <br> 5. 서비스 사용 병원 지도 : 청각장애인이 수어 통역사 등 제 3자 없이도 진료를 받을 수 있는 병원을 쉽게 찾을 수 있도록, 서비스를 도입한 병원의 위치를 지도에 표시하여 제공합니다. |
| (4) 기대효과 및 의의 | 본 서비스(handDoc)는 청각장애인이 진료 과정에서 겪어온 실질적인 제약을 직접적으로 해소합니다. 우선 대면 진료에서는, 기존에는 반드시 수어통역사나 보호자가 동석해야 원활한 대화가 가능했으나 handDoc를 통해 환자와 의사가 제 3자의 개입 없이 직접 소통할 수 있습니다. 또한 비대면 진료 환경의 경우, 그동안 청각장애인은 사실상 이용이 불가능하거나 제한적이었으나 handDoc은 실시간 수어 인식과 음성 자막화를 지원함으로써 일반 환자와 동일하게 원격 진료에 참여할 수 있도록 합니다. 이는 단순한 편의성 향상을 넘어, 청각장애인의 원격의료 접근권 보장이라는 가능성을 열어줍니다.  |  
| (5) 주요 기능 리스트 | **1. 대면 진료 환경에서의 활용** <br> 1.1 환자의 수어 인식 및 번역 : 환자가 사용하는 수어를 실시간으로 텍스트로 변환합니다.  <br> 1.2 의사 음성 자막화 : 의사의 음성이 텍스트화되어 환자에게 제공됩니다. <br> 1.3 진료 내용 요약 제공 : 진료가 완료된 후에는 대화 내역과 함께 요약이 제공됩니다. <br><br>  **2. 비대면 진료 환경에서의 활용** <br> 비대면 진료에서는 대면 진료 환경에서 제공하는 기능에 더해, 환자의 상태와 의사소통 방식에 따라 맞춤형 기능을 제공합니다. 환자는 진료 신청 단계에서 수어 사용 여부 및 발화 가능 여부를 선택할 수 있으며, 이에 따라 진료 방식이 조정됩니다. <br> 2.1 환자의 수어 인식 및 번역 : 수어 중심 환자의 경우, 실시간 수어 인식과 텍스트 변환은 통해 진료 참여를 지원합니다. <br> 2.2 의사 음성 자막화 : 의사의 음성이 텍스트화되어 환자에게 제공됩니다. 2.3 음성을 활용할 수 있는 청각장애인을 위한 음성 텍스트화 :  발음이 어눌한 청각장애인의 발화를 인식한 다음 3가지 후보 문장을 제시하고, 환자가 원하는 표현을 선택할 수 있도록 지원합니다. <br> 2.4 진료 내용 요약 제공 : 진료가 완료된 후에는 대화 내역과 함께 요약이 제공됩니다. <br> 2.5 진료 내역 조회 : 진료 내역 탭에서 본인의 진료 내역과 대화 내용, 요약 조회가 가능합니다. <br><br>   **3. 서비스 사용 병원 조회**  : 본 서비스를 사용하는 병원을 지도에 표시하여 위치 정보를 조회할 수 있는 기능을 제공합니다. | 

<br> 

## Project-Design & Implementation
| 항목 | 내용 |
|:---  |---  |
| (1) 요구사항 정의 | **1. 기능명세서** <br>  <img width="400" height="600" alt="image" src="https://github.com/user-attachments/assets/2361241c-40c1-48da-9f39-ba8c70310079" /> <img width="400" height="600" alt="image" src="https://github.com/user-attachments/assets/ea086c8e-2737-4a78-a48f-a8187fe788e3" /> <br> **2. ERD** <br> <img width="600" height="400" alt="image" src="https://github.com/user-attachments/assets/5297e5a5-25ac-41a3-b290-a5b0b87eb4b6" /> <br> **3. 서비스 플로우** <br> <img width="1000" height="450" alt="image" src="https://github.com/user-attachments/assets/56931da6-1166-42e2-8261-525111cfb03e" /> | 
| (2) 전체 시스템 구성 | <img width="600" height="350" alt="image" src="https://github.com/user-attachments/assets/3f9ff667-07b3-4d7c-a6f2-2a2a3b644fb9" />|
| (3) 주요 엔진 및 기능 설계 | 1. 프론트엔드 : React 기반으로 구성되어 있으며, WebRTC 화상 통신, Websocket 연결, 실시간 자막 출력 등의 사용자 인터페이스를 담당한다. <br> 2. 백엔드 : Spring Boot 기반의 백엔드 서버로, 사용자 인증, WebRTC 시그널링, 데이터 관리 등의 기능을 담당한다. <br> 3. 수어 인식 AI : FastAPI 기반으로 실시간 WebSocket 통신을 처리하며, 프론트엔드로부터 영상 프레임을 수신한 후, 수어 인식 모델을 통해 결과를 반환한다. <br> 4. 음성 인식 AI : FastAPI 기반으로 음성 파일을 받아 파인튜닝된 Whisper 모델을 통해 텍스트로 결과를 반환한다.  <br> 5. 인프라 : AWS EC2 1대에 Docker를 설치하고 그 위에 Nginx와 3개의 애플리케이션을 컨테이너로 실행하는 구조입니다.  |
| (4) 주요 기능의 구현 | **1. 환자의 수어 텍스트화** : 프론트엔드는 환자의 웹 브라우저에서 카메라를 활성화하여 실시간 영상 입력을 받으며, 이를 프레임 단위로 캡처합니다. 캡처된 프레임 데이터는 WebSocket 연결을 통해 FastAPI 서버로 실시간 전송됩니다. WebSocket은 연결을 한 번 맺으면 끊지 않고 계속해서 데이터를 보낼 수 있는 실시간 통로 역할을 하여, 환자의 동작을 끊김 없이 전달할 수 있습니다. FastAPI 서버는 이 WebSocket 통로를 통해 실시간으로 전송되는 영상 프레임을 순차적으로 수신하고, 프레임이 도착하는 즉시 Mediapipe를 활용하여 수어 동작의 키포인트를 추출합니다. 이 키포인트 데이터는 30프레임으로 묶여 Bi-LSTM 딥러닝 모델의 입력으로 전달되며, 모델은 이 연속된 동작을 분석하여 최종 텍스트로 변환합니다. <br><br> **2. 의사의 음성 텍스트화** : 프론트엔드는 진료 중 의사의 음성 데이터를 녹음합니다. 녹음된 음성 파일은 axios를 통해 백엔드 서버로 전송됩니다. 백엔드에서는 Nginx를 거쳐 이 음성 데이터를 수신하고, 전달받은 음성 파일을 Naver CLOVA Speech-to-Text API로 전달하여 텍스트 변환을 요청합니다. 변환된 텍스트 결과를 반환하면, 백엔드 서버는 이 텍스트 데이터를 받아 클라이언트로 전송합니다. <br><br> **3. 수어가 아닌 구음을 사용하는 환자의 음성 텍스트화** : 프론트엔드는 환자의 음성을 녹음합니다. 이 녹음된 음성 데이터는 axios를 통해 백엔드 서버로 전송되고, Nginx 웹 서버를 거쳐 이 기능을 전담하는 FastAPI 서버로 전달됩니다. 이 FastAPI 서버에서는 파인튜닝을 진행한 Whisper 모델이 탑재되어 있습니다. 서버는 프론트엔드로부터 전달받은 음성 데이터를 청각장애인 발화 변환에 특화된 Whisper 모델에 입력하여, 환자의 음성을 정확한 텍스트로 변환한 결과를 반환합니다. 변환된 텍스트는 OpenAI gpt-4o모델에 입력되어, 더 명확하게 다듬은 3가지 후보군을 생성합니다. 환자는 다음 3가지 후보군 중에서 하나를 선택하여 전송할 수 있습니다. 여기서는 프롬프팅 엔지니어링(Prompt Engineering) 기법이 사용됩니다. OpenAI gpt-4o 모델을 활용하여 Whisper가 번역한 텍스트를 3가지 보정 후보군으로 정제합니다. 이 과정에서 AI가 정확히 의도한대로 작동하도록, 시스템 프롬프트와 사용자 프롬프트를 조합하는 정교한 프롬프팅 엔지니어링을 적용했습니다. <br><br> **4. WebRTC 화상 통화 연결** : WebRTC 연결을 위해서는 2가지 종류의 핵심 통신이 필요합니다. 실제 통화 전에 서로의 정보를 교환하는 시그널링 (Signaling)과 연결이 성사된 후 실제 영상과 음성을 주고 받는 P2P (Peer-to-Peer) 통신입니다.  백엔드 서버는 의사와 환자 클라이언트가 P2P 연결을 맺는데 필요한 핵심 연결 정보(SDP, ICE Candidates)를 교환할 수 있도록 중개하는 시그널링 역할만 담당합니다. 서로의 정보를 성공적으로 교환한 이후, 클라이언트는 이 정보를 바탕으로 서버를 거치지 않고 직접적인 P2P 연결을 수립하여, 이후의 실제 영상과 음성 미디어 스트림은 클라이언트 간에 직접 오가도록 합니다. <br><br> **5. 진료 내용 요약** : 진료가 진행되는 동안, 의사와 환자 간의 모든 대화 내용은 데이터베이스의 MongoDB에 저장됩니다. 진료가 종료된 후, 의사와 환자 간의 전체 대화 내용을 OpenAI gpt-4o 모델에 전달하여 3가지 핵심 항목으로 요약하는 기능이 수행됩니다. 이 과정에서 일관되고 정확한 결과물을 얻기 위해 다음과 같은 프롬프팅 엔지니어링 전략을 사용했습니다. | 
| (5) 기타 | **1. 딥러닝 기반 수어 인식 모델** : 수어를 사용하는 청각장애인 환자의 의사소통을 지원하기 위해 수어 인식(Sign-to-Text) AI를 구축하였습니다. 본 기능은 handDoc 서비스 내에서 수어 사용자 환자가 자신의 수어 동작을 텍스트로 변환하여 의사에게 전달할 수 있도록 하는 핵심 기술입니다. Google MediaPipe Holistic을 활용해 수어 영상으로부터 상반신의 좌표(keypoints)를 추출하고, 이를 기반으로 1D-CNN / LSTM / CNN+LSTM / BiLSTM 네 가지 모델을 각각 학습시켜 10개 단어를 인식할 수 있도록 하였습니다. 이 중 가장 높은 정확도(Val Acc 98.9%)를 보인 BiLSTM 모델을 최종 선정하였습니다. <br><br> **2. 파인튜닝한 Whisper 모델** : 청각장애인 중에는 구음을 주요한 의사소통 수단으로 사용하는 분들도 있습니다. 그러나 이들의 발화는 비장애인의 표준 발화와 음성학적 특성이 달라, 기존 음성 인식 모델이 정확히 인식하지 못하는 경우가 많습니다. 따라서 ‘구음장애 음성’이라는 특정 도메인에 대해 Whisper 모델을 파인튜닝하여, 구음 장애가 있는 청각장애인의 음성을 보다 정확하게 텍스트로 변환하는 음성 인식 모델을 구축하는 것을 목표로 합니다. 모델의 최종 성능을 높이기 위해, QLoRA의 핵심 하이퍼파라미터의 Rank값을 변경하며 비교 실험을 진행했습니다. r은 LoRA 어댑터의 복잡도를 결정하며, 이 값이 높을수록 더 많은 파라미터를 학습할 수 있습니다. 실험에서는 r=8과 r=16을 설정하여 두 어댑터의 성능을 비교했습니다. Rank값이 변경됨에 따라, LoRA의 스케일링 파라미터인 lora_alpha 값도 어댑터의 영향력을 일관되게 유지하기 위해 각 각 32와 64로 비례하여 조정했습니다. 이 두 값을 제외한 다른 모든 주요 훈련 설정은 동일하게 통제했습니다. 두 모델 간 성능 차이는 크지 않았지만, r=8 모델이 미세하게 WER과 CER 두 지표에서 모두, 더 우수한 (낮은) 성능을 보였습니다. 어댑터의 복잡도(Rank)를 r=16으로 늘려 파라미터 수를 2배로 증가시켰음에도 성능 향상이 미미했기 때문에, r=8 모델을 최종 모델로 선정하여 서비스에 적용했습니다.| 

